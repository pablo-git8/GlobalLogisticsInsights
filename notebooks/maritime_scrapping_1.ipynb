{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the variables from .env into the environment\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the News Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.maritimelogisticsprofessional.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(5)\n",
    "browser.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.31\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.32\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.33\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.34\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.35\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.36\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.37\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.38\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.39\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.40\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"bc13b25fbeeaaa07438b45a3b3f66296\", element=\"f.1F6728003F0A99EF7C8BEE4B1F22756D.d.3AB28E677831FE7E401C233B0EEBA3DA.e.41\")>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of latest news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "latest_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.maritimeprofessional.com/news/houthi-rockets-wreak-havoc-middle-390817'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Article\n",
    "article_link = latest_news[0].get_attribute('href')\n",
    "article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "time.sleep(5)\n",
    "browser.get(article_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Port of Long Beach Opens New Fireboat Stations\n",
      "Article Text: Officials dedicated two new Port of Long Beach fireboat stations that are enhancing the Long Beach Fire Department’s waterside and landside emergency response capabilities, better safeguarding visiting ships, cargo and waterfront workers. The facilities – Fireboat Station 15 and Fireboat Station 20 – are the products of a $109 million Port of Long Beach program to preserve business continuity, security and economic interests. With an anticipated lifespan of 50 years, both stations were approved for construction in 2017 by the Long Beach Board of Harbor Commissioners and funded by Port revenues. “These fireboat stations will provide an important and invaluable safety service to our Port,” said Long Beach Mayor Rex Richardson. “Our firefighters are now equipped with the most sophisticated facilities and emergency response capabilities to protect our Port, its valuable assets and our waterfront workforce.” “This is a great day for the Port of Long Beach, as we celebrate completion of a long-needed major revamping of fire safety facilities and equipment in the harbor,” said Port of Long Beach CEO Mario Cordero. “Fireboat Stations 15 and 20 and our fireboats vastly improve our preparedness and resilience in the face of emergencies.” “These new stations will have a lasting and positive impact on our ability to quickly respond to emergencies,” said Long Beach Harbor Commission President Bobby Olvera Jr. “We look forward to our continuing partnership with the Long Beach Fire Department to make the Port a safe place to work and do business.” “Our new fire stations are state-of-the-art public safety structures that will serve as bases of operations for any incident within the Port and throughout the region,” said Long Beach Fire Chief Dennis Buchanan. “Additionally, these stations have been designed with today’s workforce in mind, which means the assurance of workforce accommodations, such as separate sleeping quarters and restrooms.” Fireboat Station 15 is a single-level, 7,750-square-foot building in the Port’s outer harbor with living quarters, a garage for two firefighting apparatus trucks and a full wharf with a 16,311-square-foot boat bay enclosure that houses fireboat Vigilance. Construction started in April 2019 and the project was completed in September 2021. Fireboat Station 20, located in the Port’s inner harbor, is a two-level, 9,783-square-foot structure equipped with living quarters, a garage for two firefighting apparatus trucks and a 16,280-square-foot boat bay enclosure that houses fireboat Protector. Construction started in March 2021 and the project was completed in December 2023. Fireboat Protector entered service in 2016, followed a year later by its companion, Vigilance, heralding major advancements in harbor firefighting and emergency response capabilities at the Port of Long Beach. The fireboats are each equipped with 10 water cannons capable of sending up to 41,000 gallons per minute to a distance of up to 600 feet, or the length of two football fields.\n"
     ]
    }
   ],
   "source": [
    "# Find the title element and get the text\n",
    "title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "article_title = title_element.text\n",
    "\n",
    "# Find the article body element and get all the paragraph texts\n",
    "article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "# Combine the text of all paragraphs to form the body text\n",
    "article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "# Now you have the article's title and text\n",
    "print(\"Title:\", article_title)\n",
    "print(\"Article Text:\", article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all URLs before navigating\n",
    "urls = [element.get_attribute('href') for element in latest_news]\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Getting {url}\")\n",
    "    time.sleep(5)  # Consider using WebDriverWait here instead of time.sleep for better efficiency\n",
    "    browser.get(url)\n",
    "    # Now you can perform your scraping logic on each article page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords from JSON file\n",
    "with open('../json/keywords.json', 'r') as file:\n",
    "    keywords = json.load(file)\n",
    "\n",
    "def classify_article(article_text, keywords):\n",
    "    max_count = 0\n",
    "    max_category = \"Unclassified or Neutral\"\n",
    "\n",
    "    # Convert article text to lower case for comparison\n",
    "    article_text_lower = article_text.lower()\n",
    "    \n",
    "    # Check for the presence of each keyword in the article\n",
    "    for category, category_keywords in keywords.items():\n",
    "        count = sum(keyword in article_text_lower for keyword in category_keywords)\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_category = category\n",
    "\n",
    "    return max_category\n",
    "\n",
    "# Usage example:\n",
    "classification = classify_article(article_text, keywords)\n",
    "print(f\"The article is related to: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt= f\"\"\"\n",
    "Please summarize the key points of the article for a business audience in a concise paragraph, limiting the summary to no more than 350 characters. Then, in a separate paragraph of 500 characters, elaborate on the potential impact of the situation described in the article on maritime logistics, port operations, and supply chain management, specifically focusing on its implications for Latin America. Label this second paragraph 'Impacto en LATAM:' and ensure there is a clear separation between the two sections. Present your response in Spanish and format it as markdown text for clarity:\\n\\n{text}\n",
    "                \"\"\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "#summary = summarize_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"Get Premium for enabling AI-powered summary!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Article, Title and Summary in DDBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes a connection to the SQLite database\n",
    "def connect_to_db(db_path):\n",
    "    return sqlite3.connect(db_path)\n",
    "\n",
    "# Create a new table for storing only daily articles\n",
    "def create_daily_table(cursor, table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        title TEXT,\n",
    "                        text TEXT,\n",
    "                        summary TEXT,\n",
    "                        classification TEXT\n",
    "                    );\"\"\")\n",
    "\n",
    "# Create a new table for storing daily links\n",
    "def create_daily_links_table(cursor, table_name, articles_table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        article_id INTEGER,\n",
    "                        title TEXT,\n",
    "                        link TEXT,\n",
    "                        FOREIGN KEY (article_id) REFERENCES {articles_table_name}(id)\n",
    "                    );\"\"\")\n",
    "\n",
    "# Inserts an article into the daily table\n",
    "def insert_article_data(cursor, table_name, article_title, article_text, summary, classification):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if an article with the same title already exists\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE title = ?\", (article_title,))\n",
    "    existing_article = cursor.fetchone()\n",
    "\n",
    "    if existing_article == None:\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (title, text, summary, classification)\n",
    "                        VALUES (?, ?, ?, ?);\"\"\",\n",
    "                    (article_title, article_text, summary, classification))\n",
    "        return cursor.lastrowid  # Return the ID of the new article\n",
    "    else:\n",
    "        return existing_article[0]  # Return the ID of the existing article\n",
    "\n",
    "# Inserts a link into the daily links table\n",
    "def insert_link_data(cursor, table_name, article_id, article_title, link):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if the link already exists for the given article\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE article_id = ? AND link = ?\", (article_id, link))\n",
    "    existing_link = cursor.fetchone()\n",
    "\n",
    "    if existing_link is None:\n",
    "        # If the link does not exist for this article, insert it\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (article_id, title, link)\n",
    "                           VALUES (?, ?, ?);\"\"\",\n",
    "                       (article_id, article_title, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the SQLite database\n",
    "db_path = '../data/news/maritime_news.db'\n",
    "\n",
    "# Define table naming\n",
    "current_date = datetime.now().strftime(\"%m%d%Y\")\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for today's date with news\n",
    "news_table_name = f\"news_{current_date}\"\n",
    "create_daily_table(cursor, news_table_name)\n",
    "\n",
    "# Create table for storing links\n",
    "links_table_name = f\"news_links_{current_date}\"\n",
    "create_daily_links_table(cursor, links_table_name, news_table_name)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB News/Links Input or Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert the article data into the table and get the article ID\n",
    "create_daily_table(cursor, news_table_name)\n",
    "article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification)\n",
    "\n",
    "# Insert the link data into the links table\n",
    "insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a browser session\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(2)\n",
    "browser.get(URL)\n",
    "\n",
    "# Cookies button\n",
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLOBAL LATEST NEWS\n",
    "\"\"\"\n",
    "\n",
    "# List of latest global news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "# Collect all URLs before navigating\n",
    "article_urls = [element.get_attribute('href') for element in latest_news]\n",
    "# Premium version\n",
    "premium = False\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for article in article_urls:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get article\n",
    "    time.sleep(5)\n",
    "    browser.get(article)\n",
    "\n",
    "    # Find the title element and get the text\n",
    "    title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "    article_title = title_element.text\n",
    "\n",
    "    # Find the article body element and get all the paragraph texts\n",
    "    article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "    article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "    # Combine the text of all paragraphs to form the body text\n",
    "    article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "    # AI-powered Summary\n",
    "    if premium:\n",
    "        summarize_text(article_text)\n",
    "    else:\n",
    "        # Summary not premium\n",
    "        summary = \"Get Premium for enabling AI-powered summary!\"\n",
    "    \n",
    "    # Article classification\n",
    "    classification = classify_article(article_text, keywords)\n",
    "\n",
    "    # Insert the article data into the table and get the article ID\n",
    "    article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification)\n",
    "    # Insert the link data into the links table\n",
    "    insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "# Close browser and database connection\n",
    "browser.quit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SOUTH AMERICA NEWS\n",
    "\"\"\"\n",
    "filter_news = '/south-america'\n",
    "\n",
    "# Access the '.cat-link' (category links) elements\n",
    "cat_links = browser.find_elements(By.CSS_SELECTOR, \".cat-link\")\n",
    "\n",
    "# Iterate through each 'cat-link' div\n",
    "for cat_link_div in cat_links:\n",
    "    # Find the 'a' element within the current 'cat-link' div\n",
    "    a_element = cat_link_div.find_element(By.TAG_NAME, 'a')  # Note: it's 'find_element' not 'find_elements'\n",
    "    # Get the 'href' attribute from the 'a' element\n",
    "    href = a_element.get_attribute('href')\n",
    "    if href.endswith(f\"/{filter_news}\"):\n",
    "        # Print the href\n",
    "        print(href)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
