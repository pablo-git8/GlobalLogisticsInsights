{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of Jupyter Notebooks Development for Predictive Modeling\n",
    "\n",
    "*1_problem_statement.ipynb*\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "By analyzing a combination of supply chain dynamics, shipping times, carriers, supplier locations, production volumes, routes, and shipped product features, and providing supply chain teams with a chatbot for easy real-time tracking of shipments, along with an integrated tool to stay up-to-date with relevant news from various webpages, we can predict transportation costs and enhance decision-making towards supply chain expense management while keeping operations aligned with the latest industry standards and insights.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "For our proof of concept, we're utilizing a comprehensive dataset dedicated to supply chain analysis, which is accessible on [Kaggle](https://www.kaggle.com/datasets/harshsingh2209/supply-chain-analysis/download?datasetVersionNumber=1). This dataset serves as the foundational framework for the ongoing development, testing, and subsequent implementation of ETL (Extract, Transform, Load) pipelines. These pipelines will be tailored and integrated with actual customer databases once we secure the necessary data access permissions.\n",
    "\n",
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2_data_wrangling.ipynb*\n",
    "\n",
    "## Stages:\n",
    "\n",
    "1. Handle missing values\n",
    "2. Define categorical features\n",
    "3. Perform feature engineering\n",
    "4. List insights for the Exploratory Data Analysis\n",
    "5. Define the data transformations needed\n",
    "\n",
    "## Output:\n",
    "\n",
    "Dataset prepared for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3_EDA.ipynb*\n",
    "\n",
    "## Stages:\n",
    "\n",
    "1. Data Wrangling Dataset Ingestion\n",
    "2. Analyze categorical and numerical features\n",
    "3. Select features based on their correlations\n",
    "4. Select features and the target variable\n",
    "5. Examine the distribution of numerical features\n",
    "6. Select features based on their correlations\n",
    "7. Re-define steps in data wrangling stages (if applicable)\n",
    "8. Clean the dataset for modeling\n",
    "\n",
    "## Output:\n",
    "\n",
    "Dataset for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*4_modeling.ipynb*\n",
    "\n",
    "## Stages:\n",
    "\n",
    "1. EDA Dataset Ingestion\n",
    "2. Choose Model Type\n",
    "3. Train/Test Phase\n",
    "4. Save Intermediate Datasets\n",
    "5. Model Evaluation Metrics\n",
    "6. Try Different ML Models\n",
    "7. Pick a Useful Metric\n",
    "8. Condense Models and Metrics\n",
    "9. Visualization of Performance Plots\n",
    "10. Saving the Model\n",
    "\n",
    "## Output:\n",
    "\n",
    ".pkl file model for later usage in pipelines and platform integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deploment*\n",
    "\n",
    "Predictive feature integration with software platform."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
